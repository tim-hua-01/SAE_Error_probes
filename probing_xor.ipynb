{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What was what something they found within buddy house who about something about buddy house?\\n\\n  gold, at the pot of craft of direction, that was looking. This diagues was hidden, as other characters. “but if anything was expected”.\\n\\n  beyond of the natural scene, the conction of buddy was that. buddy intrusted raine, the lethiphness may go hot to happy.\\n\\n  Buddy\\'s sequences were launched administrate. Buddy\\'s looked progtamp and introduced the moured from the chater floor.\\n\\n  This is the only boat, which is with a certain of chars. They are freeds and conderetained.\\n\\n  with all that buddy is doing, was the apperances of another treasure Hunters, deas. “this is the mission of the tend of this blue wire”.\\n\\n  They measured out cooperative tools. Some equipment. why he have stopped into buddy house of mothy of lakes?\\n\\n  with waiting explover, was slow a word on the floor. buddy did not want to have a approach kind about, the decision was inshadower.\\n\\n  They also laerned what they were really doing. What was this the prethord of undunder.\\n\\n  Buddy\\'s a block old shelves and sand behind. Was almost waiting little, the cat was already for a hunt.\\n\\n  They cleader the flooring. I think there is a columbin, in the shape of softs groud, easily was already taped on the ground.\\n\\n  buddy laxemied \"I\\'ve found the piocec of gold, it is burried and here\". They looked over schotting back beck, looking like a whattack, it was filled with a budle of wheat.\\n\\n  Mostly possession of Maringregrand\\'s years, it was a wonderful Myster. They bought into the house, the cat said “little girl, you need to get out of your house”.\\n\\n  This was the end of hteir adventure. it in anonother pocket. It almost, one of the yards would never forget that. They couldn\\'t see easily, but have sunlights filter wently soothing through the gradress, there was a tfash of light of a green power.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\".rewop neerg a fo thgil fo hsaft a saw ereht ,sserdarg eht hguorht gnihtoos yltnew retlif sthgilnus evah tub ,ylisae ees t'ndluoc yehT .taht tegrof reven dluow sdray eht fo eno ,tsomla tI .tekcop rehtonona ni ti .erutnevda rieth fo dne eht saw sihT  \n",
    "\n",
    ".”esuoh ruoy fo tuo teg ot deen uoy ,lrig elttil“ dias tac eht ,esuoh eht otni thguob yehT .retsyM lufrednow a saw ti ,sraey s'dnargergniraM fo noissessop yltsoM  \n",
    "\n",
    ".taehw fo eldub a htiw dellif saw ti ,kcattahw a ekil gnikool ,kceb kcab gnittohcs revo dekool yehT .\"ereh dna deirrub si ti ,dlog fo cecoip eht dnuof ev'I\" deimexal yddub  \n",
    "\n",
    ".dnuorg eht no depat ydaerla saw ylisae ,duorg stfos fo epahs eht ni ,nibmuloc a si ereht kniht I .gniroolf eht redaelc yehT  \n",
    "\n",
    ".tnuh a rof ydaerla saw tac eht ,elttil gnitiaw tsomla saW .dniheb dnas dna sevlehs dlo kcolb a s'ydduB  \n",
    "\n",
    ".rednudnu fo drohterp eht siht saw tahW .gniod yllaer erew yeht tahw denreal osla yehT  \n",
    "\n",
    ".rewodahsni saw noisiced eht ,tuoba dnik hcaorppa a evah ot tnaw ton did yddub .roolf eht no drow a wols saw ,revolpxe gnitiaw htiw  \n",
    "\n",
    "?sekal fo yhtom fo esuoh yddub otni deppots evah eh yhw .tnempiuqe emoS .sloot evitarepooc tuo derusaem yehT  \n",
    "\n",
    ".”eriw eulb siht fo dnet eht fo noissim eht si siht“ .saed ,sretnuH erusaert rehtona fo secnareppa eht saw ,gniod si yddub taht lla htiw  \n",
    "\n",
    ".deniaterednoc dna sdeerf era yehT .srahc fo niatrec a htiw si hcihw ,taob ylno eht si sihT  \n",
    "\n",
    ".roolf retahc eht morf deruom eht decudortni dna pmatgorp dekool s'ydduB .etartsinimda dehcnual erew secneuqes s'ydduB  \n",
    "\n",
    ".yppah ot toh og yam ssenhpihtel eht ,eniar detsurtni yddub .taht saw yddub fo noitcnoc eht ,enecs larutan eht fo dnoyeb  \n",
    "\n",
    ".”detcepxe saw gnihtyna fi tub“ .sretcarahc rehto sa ,neddih saw seugaid sihT .gnikool saw taht ,noitcerid fo tfarc fo top eht ta ,dlog  \n",
    "\n",
    "?esuoh yddub tuoba gnihtemos tuoba ohw esuoh yddub nihtiw dnuof yeht gnihtemos tahw saw tahW\"\"\"[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87507b56a1d94d8bb9f5f7bd4c9ea3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model2b = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>correct_country</th>\n",
       "      <th>has_alice</th>\n",
       "      <th>has_not</th>\n",
       "      <th>has_alice xor has_not</th>\n",
       "      <th>has_alice xor label</th>\n",
       "      <th>has_not xor label</th>\n",
       "      <th>has_alice xor has_not xor label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice: The city of Hegang is in China.</td>\n",
       "      <td>1</td>\n",
       "      <td>Hegang</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob: The city of Cangzhou is in China.</td>\n",
       "      <td>1</td>\n",
       "      <td>Cangzhou</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice: The city of Mogadishu is in Somalia.</td>\n",
       "      <td>1</td>\n",
       "      <td>Mogadishu</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob: The city of Laibin is in Ethiopia.</td>\n",
       "      <td>0</td>\n",
       "      <td>Laibin</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>China</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob: The city of Feira de Santana is in Brazil.</td>\n",
       "      <td>1</td>\n",
       "      <td>Feira de Santana</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alice: The city of Karachi is not in Pakistan.</td>\n",
       "      <td>0</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alice: The city of Luohe is in Brazil.</td>\n",
       "      <td>0</td>\n",
       "      <td>Luohe</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bob: The city of Tlalpan is not in Mexico.</td>\n",
       "      <td>0</td>\n",
       "      <td>Tlalpan</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bob: The city of Bekasi is in China.</td>\n",
       "      <td>0</td>\n",
       "      <td>Bekasi</td>\n",
       "      <td>China</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alice: The city of Langfang is not in India.</td>\n",
       "      <td>1</td>\n",
       "      <td>Langfang</td>\n",
       "      <td>India</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         statement  label              city  \\\n",
       "0           Alice: The city of Hegang is in China.      1            Hegang   \n",
       "1           Bob: The city of Cangzhou is in China.      1          Cangzhou   \n",
       "2      Alice: The city of Mogadishu is in Somalia.      1         Mogadishu   \n",
       "3          Bob: The city of Laibin is in Ethiopia.      0            Laibin   \n",
       "4  Bob: The city of Feira de Santana is in Brazil.      1  Feira de Santana   \n",
       "5   Alice: The city of Karachi is not in Pakistan.      0           Karachi   \n",
       "6           Alice: The city of Luohe is in Brazil.      0             Luohe   \n",
       "7       Bob: The city of Tlalpan is not in Mexico.      0           Tlalpan   \n",
       "8             Bob: The city of Bekasi is in China.      0            Bekasi   \n",
       "9     Alice: The city of Langfang is not in India.      1          Langfang   \n",
       "\n",
       "    country correct_country  has_alice  has_not  has_alice xor has_not  \\\n",
       "0     China           China       True    False                   True   \n",
       "1     China           China      False    False                  False   \n",
       "2   Somalia         Somalia       True    False                   True   \n",
       "3  Ethiopia           China      False    False                  False   \n",
       "4    Brazil          Brazil      False    False                  False   \n",
       "5  Pakistan        Pakistan       True     True                  False   \n",
       "6    Brazil           China       True    False                   True   \n",
       "7    Mexico          Mexico      False     True                   True   \n",
       "8     China       Indonesia      False    False                  False   \n",
       "9     India           China       True     True                  False   \n",
       "\n",
       "   has_alice xor label  has_not xor label  has_alice xor has_not xor label  \n",
       "0                False               True                            False  \n",
       "1                 True               True                             True  \n",
       "2                False               True                            False  \n",
       "3                False              False                            False  \n",
       "4                 True               True                             True  \n",
       "5                 True               True                            False  \n",
       "6                 True              False                             True  \n",
       "7                False               True                             True  \n",
       "8                False              False                            False  \n",
       "9                False              False                             True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_alice = pd.read_csv(\"cities_alice.csv\")\n",
    "neg_city_alice = pd.read_csv(\"neg_cities_alice.csv\")\n",
    "data = pd.concat([city_alice, neg_city_alice])\n",
    "#scramble rows\n",
    "np.random.seed(123)\n",
    "data = data.sample(frac = 1).reset_index(drop = True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2992, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_pytorch_tanh',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 16.0,\n",
       " 'attn_scores_soft_cap': 50.0,\n",
       " 'attn_types': ['global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local'],\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 256,\n",
       " 'd_mlp': 9216,\n",
       " 'd_model': 2304,\n",
       " 'd_vocab': 256000,\n",
       " 'd_vocab_out': 256000,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': 'cuda',\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-06,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'n_ctx': 8192,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 8,\n",
       " 'n_key_value_heads': 4,\n",
       " 'n_layers': 26,\n",
       " 'n_params': 2146959360,\n",
       " 'normalization_type': 'RMSPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'Gemma2ForCausalLM',\n",
       " 'output_logits_soft_cap': 30.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000.0,\n",
       " 'rotary_dim': 256,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'google/gemma-2-2b',\n",
       " 'tokenizer_prepends_bos': True,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': True,\n",
       " 'use_normalization_before_and_after': True,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': 4096}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2b.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = \"layer_19/width_16k/canonical\",\n",
    "    device = \"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247aa605c4774522b4e15f7cdba39715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Bing bong ding dong, I'm blowing this one, 9\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model2b.generate(\"Bing bong ding dong\", max_new_tokens = 10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_batch = model2b.tokenizer([\"Ding don bing bong\", \"King kong ding dong whoop whoop\"], return_tensors = \"pt\", padding = True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "leb = model2b.run_with_cache_with_saes(some_batch['input_ids'],saes = sae, names_filter = lambda name: name in ['blocks.19.hook_resid_post.hook_sae_recons', 'blocks.19.hook_resid_post.hook_sae_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
       "          -6.2675e+01,  2.9702e+01],\n",
       "         [-4.8884e+00, -6.8970e-01,  4.2473e-01,  ...,  2.6384e+00,\n",
       "           7.4499e-01,  1.6682e-01],\n",
       "         [-3.9028e+00, -8.6650e-01, -4.0607e+00,  ...,  1.5525e+00,\n",
       "           5.1970e+00, -1.1291e-01],\n",
       "         [-5.4336e+00,  1.4239e+00,  2.3322e+00,  ...,  1.5917e+00,\n",
       "          -5.3451e-01,  2.7083e+00],\n",
       "         [-9.6548e+00,  3.2607e+00,  4.4400e+00,  ..., -2.2930e+00,\n",
       "           1.9895e+00,  2.9883e+00]],\n",
       "\n",
       "        [[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
       "          -6.2675e+01,  2.9702e+01],\n",
       "         [-6.8911e+00,  8.6366e+00, -2.9912e+00,  ...,  7.0088e-01,\n",
       "          -1.5728e+00,  1.3779e+00],\n",
       "         [-4.2630e+00,  1.3513e+00, -2.4401e+00,  ..., -2.6403e+00,\n",
       "          -4.0709e+00,  3.4806e+00],\n",
       "         [-3.7446e+00, -6.9691e-01, -8.2747e-01,  ...,  5.4570e+00,\n",
       "          -1.1321e+00,  3.9979e-01],\n",
       "         [-6.1591e+00,  2.7186e+00, -5.2874e-01,  ..., -1.4442e+00,\n",
       "          -2.0987e+00,  1.7930e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leb[1]['blocks.19.hook_resid_post.hook_sae_recons']\n",
    "# tensor([[[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
    "        #   -6.2675e+01,  2.9702e+01],\n",
    "        #  [-4.8884e+00, -6.8970e-01,  4.2473e-01,  ...,  2.6384e+00,\n",
    "        #    7.4499e-01,  1.6682e-01],\n",
    "        #  [-3.9028e+00, -8.6650e-01, -4.0607e+00,  ...,  1.5525e+00,\n",
    "        #    5.1970e+00, -1.1291e-01],\n",
    "        #  [-5.4336e+00,  1.4239e+00,  2.3322e+00,  ...,  1.5917e+00,\n",
    "        #   -5.3451e-01,  2.7083e+00],\n",
    "        #  [-9.6548e+00,  3.2607e+00,  4.4400e+00,  ..., -2.2930e+00,\n",
    "        #    1.9895e+00,  2.9883e+00]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to('cuda')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(batches):\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other people's code\n",
    "\n",
    "labels = [\n",
    "    'has_alice',\n",
    "    'has_not',\n",
    "    'label',\n",
    "    'has_alice xor has_not',\n",
    "    'has_alice xor label',\n",
    "    'has_not xor label',\n",
    "    'has_alice xor has_not xor label',\n",
    "]\n",
    "    \n",
    "accs = {}\n",
    "for label in labels:\n",
    "    dm = DataManager()\n",
    "    for dataset in ['cities_alice', 'neg_cities_alice']:\n",
    "        dm.add_dataset(dataset, 'llama-2-13b-reset', 14, label=label, center=False, split=0.8)\n",
    "    acts, labels = dm.get('train')\n",
    "    probe = LRProbe.from_data(acts, labels, bias=True)\n",
    "    acts, labels = dm.get('val')\n",
    "    acc = (probe(acts).round() == labels).float().mean()\n",
    "    accs[label] = acc"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
