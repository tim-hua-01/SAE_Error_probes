{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbetas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight_decay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaximize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcapturable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Implements AdamW algorithm.\n",
      "\n",
      ".. math::\n",
      "   \\begin{aligned}\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{input}      : \\gamma \\text{(lr)}, \\: \\beta_1, \\beta_2\n",
      "            \\text{(betas)}, \\: \\theta_0 \\text{(params)}, \\: f(\\theta) \\text{(objective)},\n",
      "            \\: \\epsilon \\text{ (epsilon)}                                                    \\\\\n",
      "        &\\hspace{13mm}      \\lambda \\text{(weight decay)},  \\: \\textit{amsgrad},\n",
      "            \\: \\textit{maximize}                                                             \\\\\n",
      "        &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ (first moment)}, v_0 \\leftarrow 0\n",
      "            \\text{ ( second moment)}, \\: \\widehat{v_0}^{max}\\leftarrow 0              \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      "\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n",
      "        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n",
      "        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
      "        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      "        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}         \\\\\n",
      "        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n",
      "        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n",
      "        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n",
      "        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n",
      "        &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_{t-1}}^{max},\n",
      "            \\widehat{v_t})                                                                   \\\\\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n",
      "            \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n",
      "        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n",
      "            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "   \\end{aligned}\n",
      "\n",
      "For further details regarding the algorithm we refer to `Decoupled Weight Decay Regularization`_.\n",
      "\n",
      "Args:\n",
      "    params (iterable): iterable of parameters or named_parameters to optimize\n",
      "        or iterable of dicts defining parameter groups. When using named_parameters,\n",
      "        all parameters in all groups should be named\n",
      "    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n",
      "        is not yet supported for all our implementations. Please use a float\n",
      "        LR if you are not also specifying fused=True or capturable=True.\n",
      "    betas (Tuple[float, float], optional): coefficients used for computing\n",
      "        running averages of gradient and its square (default: (0.9, 0.999))\n",
      "    eps (float, optional): term added to the denominator to improve\n",
      "        numerical stability (default: 1e-8)\n",
      "    weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
      "    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n",
      "        algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
      "        (default: False)\n",
      "    maximize (bool, optional): maximize the objective with respect to the\n",
      "        params, instead of minimizing (default: False)\n",
      "    foreach (bool, optional): whether foreach implementation of optimizer\n",
      "        is used. If unspecified by the user (so foreach is None), we will try to use\n",
      "        foreach over the for-loop implementation on CUDA, since it is usually\n",
      "        significantly more performant. Note that the foreach implementation uses\n",
      "        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n",
      "        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n",
      "        parameters through the optimizer at a time or switch this flag to False (default: None)\n",
      "    capturable (bool, optional): whether this instance is safe to\n",
      "        capture in a CUDA graph. Passing True can impair ungraphed performance,\n",
      "        so if you don't intend to graph capture this instance, leave it False\n",
      "        (default: False)\n",
      "    differentiable (bool, optional): whether autograd should\n",
      "        occur through the optimizer step in training. Otherwise, the step()\n",
      "        function runs in a torch.no_grad() context. Setting to True can impair\n",
      "        performance, so leave it False if you don't intend to run autograd\n",
      "        through this instance (default: False)\n",
      "    fused (bool, optional): whether the fused implementation is used.\n",
      "        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n",
      "        are supported. (default: None)\n",
      "\n",
      ".. note:: The foreach and fused implementations are typically faster than the for-loop,\n",
      "          single-tensor implementation, with fused being theoretically fastest with both\n",
      "          vertical and horizontal fusion. As such, if the user has not specified either\n",
      "          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n",
      "          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n",
      "          implementation is relatively new, we want to give it sufficient bake-in time.\n",
      "          To specify fused, pass True for fused. To force running the for-loop\n",
      "          implementation, pass False for either foreach or fused. \n",
      ".. Note::\n",
      "    A prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n",
      ".. _Decoupled Weight Decay Regularization:\n",
      "    https://arxiv.org/abs/1711.05101\n",
      ".. _On the Convergence of Adam and Beyond:\n",
      "    https://openreview.net/forum?id=ryQu7f-RZ\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/new/lib/python3.12/site-packages/torch/optim/adamw.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?t.optim.AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30e2bae56d645eaa1fc3cf736ed6caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model2b = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>correct_country</th>\n",
       "      <th>has_alice</th>\n",
       "      <th>has_not</th>\n",
       "      <th>has_alice xor has_not</th>\n",
       "      <th>has_alice xor label</th>\n",
       "      <th>has_not xor label</th>\n",
       "      <th>has_alice xor has_not xor label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice: The city of Hegang is in China.</td>\n",
       "      <td>1</td>\n",
       "      <td>Hegang</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob: The city of Cangzhou is in China.</td>\n",
       "      <td>1</td>\n",
       "      <td>Cangzhou</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice: The city of Mogadishu is in Somalia.</td>\n",
       "      <td>1</td>\n",
       "      <td>Mogadishu</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob: The city of Laibin is in Ethiopia.</td>\n",
       "      <td>0</td>\n",
       "      <td>Laibin</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>China</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob: The city of Feira de Santana is in Brazil.</td>\n",
       "      <td>1</td>\n",
       "      <td>Feira de Santana</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alice: The city of Karachi is not in Pakistan.</td>\n",
       "      <td>0</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alice: The city of Luohe is in Brazil.</td>\n",
       "      <td>0</td>\n",
       "      <td>Luohe</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bob: The city of Tlalpan is not in Mexico.</td>\n",
       "      <td>0</td>\n",
       "      <td>Tlalpan</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bob: The city of Bekasi is in China.</td>\n",
       "      <td>0</td>\n",
       "      <td>Bekasi</td>\n",
       "      <td>China</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alice: The city of Langfang is not in India.</td>\n",
       "      <td>1</td>\n",
       "      <td>Langfang</td>\n",
       "      <td>India</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         statement  label              city  \\\n",
       "0           Alice: The city of Hegang is in China.      1            Hegang   \n",
       "1           Bob: The city of Cangzhou is in China.      1          Cangzhou   \n",
       "2      Alice: The city of Mogadishu is in Somalia.      1         Mogadishu   \n",
       "3          Bob: The city of Laibin is in Ethiopia.      0            Laibin   \n",
       "4  Bob: The city of Feira de Santana is in Brazil.      1  Feira de Santana   \n",
       "5   Alice: The city of Karachi is not in Pakistan.      0           Karachi   \n",
       "6           Alice: The city of Luohe is in Brazil.      0             Luohe   \n",
       "7       Bob: The city of Tlalpan is not in Mexico.      0           Tlalpan   \n",
       "8             Bob: The city of Bekasi is in China.      0            Bekasi   \n",
       "9     Alice: The city of Langfang is not in India.      1          Langfang   \n",
       "\n",
       "    country correct_country  has_alice  has_not  has_alice xor has_not  \\\n",
       "0     China           China       True    False                   True   \n",
       "1     China           China      False    False                  False   \n",
       "2   Somalia         Somalia       True    False                   True   \n",
       "3  Ethiopia           China      False    False                  False   \n",
       "4    Brazil          Brazil      False    False                  False   \n",
       "5  Pakistan        Pakistan       True     True                  False   \n",
       "6    Brazil           China       True    False                   True   \n",
       "7    Mexico          Mexico      False     True                   True   \n",
       "8     China       Indonesia      False    False                  False   \n",
       "9     India           China       True     True                  False   \n",
       "\n",
       "   has_alice xor label  has_not xor label  has_alice xor has_not xor label  \n",
       "0                False               True                            False  \n",
       "1                 True               True                             True  \n",
       "2                False               True                            False  \n",
       "3                False              False                            False  \n",
       "4                 True               True                             True  \n",
       "5                 True               True                            False  \n",
       "6                 True              False                             True  \n",
       "7                False               True                             True  \n",
       "8                False              False                            False  \n",
       "9                False              False                             True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_alice = pd.read_csv(\"cities_alice.csv\")\n",
    "neg_city_alice = pd.read_csv(\"neg_cities_alice.csv\")\n",
    "data = pd.concat([city_alice, neg_city_alice])\n",
    "#scramble rows\n",
    "np.random.seed(123)\n",
    "data = data.sample(frac = 1).reset_index(drop = True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2992, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_pytorch_tanh',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 16.0,\n",
       " 'attn_scores_soft_cap': 50.0,\n",
       " 'attn_types': ['global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local'],\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 256,\n",
       " 'd_mlp': 9216,\n",
       " 'd_model': 2304,\n",
       " 'd_vocab': 256000,\n",
       " 'd_vocab_out': 256000,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': 'cuda',\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-06,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'n_ctx': 8192,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 8,\n",
       " 'n_key_value_heads': 4,\n",
       " 'n_layers': 26,\n",
       " 'n_params': 2146959360,\n",
       " 'normalization_type': 'RMSPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'Gemma2ForCausalLM',\n",
       " 'output_logits_soft_cap': 30.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000.0,\n",
       " 'rotary_dim': 256,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'google/gemma-2-2b',\n",
       " 'tokenizer_prepends_bos': True,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': True,\n",
       " 'use_normalization_before_and_after': True,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': 4096}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2b.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = \"layer_19/width_16k/canonical\",\n",
    "    device = \"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247aa605c4774522b4e15f7cdba39715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Bing bong ding dong, I'm blowing this one, 9\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model2b.generate(\"Bing bong ding dong\", max_new_tokens = 10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_batch = model2b.tokenizer([\"Ding don bing bong\", \"King kong ding dong whoop whoop\"], return_tensors = \"pt\", padding = True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "leb = model2b.run_with_cache_with_saes(some_batch['input_ids'],saes = sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.ln1_post.hook_scale', 'blocks.0.ln1_post.hook_normalized', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_pre_linear', 'blocks.0.mlp.hook_post', 'blocks.0.ln2_post.hook_scale', 'blocks.0.ln2_post.hook_normalized', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.ln1_post.hook_scale', 'blocks.1.ln1_post.hook_normalized', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_pre_linear', 'blocks.1.mlp.hook_post', 'blocks.1.ln2_post.hook_scale', 'blocks.1.ln2_post.hook_normalized', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.ln1_post.hook_scale', 'blocks.2.ln1_post.hook_normalized', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_pre_linear', 'blocks.2.mlp.hook_post', 'blocks.2.ln2_post.hook_scale', 'blocks.2.ln2_post.hook_normalized', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.ln1_post.hook_scale', 'blocks.3.ln1_post.hook_normalized', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_pre_linear', 'blocks.3.mlp.hook_post', 'blocks.3.ln2_post.hook_scale', 'blocks.3.ln2_post.hook_normalized', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.ln1_post.hook_scale', 'blocks.4.ln1_post.hook_normalized', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_pre_linear', 'blocks.4.mlp.hook_post', 'blocks.4.ln2_post.hook_scale', 'blocks.4.ln2_post.hook_normalized', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.ln1_post.hook_scale', 'blocks.5.ln1_post.hook_normalized', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_pre_linear', 'blocks.5.mlp.hook_post', 'blocks.5.ln2_post.hook_scale', 'blocks.5.ln2_post.hook_normalized', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_rot_q', 'blocks.6.attn.hook_rot_k', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.ln1_post.hook_scale', 'blocks.6.ln1_post.hook_normalized', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_pre_linear', 'blocks.6.mlp.hook_post', 'blocks.6.ln2_post.hook_scale', 'blocks.6.ln2_post.hook_normalized', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_rot_q', 'blocks.7.attn.hook_rot_k', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.ln1_post.hook_scale', 'blocks.7.ln1_post.hook_normalized', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_pre_linear', 'blocks.7.mlp.hook_post', 'blocks.7.ln2_post.hook_scale', 'blocks.7.ln2_post.hook_normalized', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_rot_q', 'blocks.8.attn.hook_rot_k', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.ln1_post.hook_scale', 'blocks.8.ln1_post.hook_normalized', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_pre_linear', 'blocks.8.mlp.hook_post', 'blocks.8.ln2_post.hook_scale', 'blocks.8.ln2_post.hook_normalized', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_rot_q', 'blocks.9.attn.hook_rot_k', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.ln1_post.hook_scale', 'blocks.9.ln1_post.hook_normalized', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_pre_linear', 'blocks.9.mlp.hook_post', 'blocks.9.ln2_post.hook_scale', 'blocks.9.ln2_post.hook_normalized', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_rot_q', 'blocks.10.attn.hook_rot_k', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.ln1_post.hook_scale', 'blocks.10.ln1_post.hook_normalized', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_pre_linear', 'blocks.10.mlp.hook_post', 'blocks.10.ln2_post.hook_scale', 'blocks.10.ln2_post.hook_normalized', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_rot_q', 'blocks.11.attn.hook_rot_k', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.ln1_post.hook_scale', 'blocks.11.ln1_post.hook_normalized', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_pre_linear', 'blocks.11.mlp.hook_post', 'blocks.11.ln2_post.hook_scale', 'blocks.11.ln2_post.hook_normalized', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_pre', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_rot_q', 'blocks.12.attn.hook_rot_k', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_z', 'blocks.12.ln1_post.hook_scale', 'blocks.12.ln1_post.hook_normalized', 'blocks.12.hook_attn_out', 'blocks.12.hook_resid_mid', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_pre_linear', 'blocks.12.mlp.hook_post', 'blocks.12.ln2_post.hook_scale', 'blocks.12.ln2_post.hook_normalized', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_pre', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_rot_q', 'blocks.13.attn.hook_rot_k', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_z', 'blocks.13.ln1_post.hook_scale', 'blocks.13.ln1_post.hook_normalized', 'blocks.13.hook_attn_out', 'blocks.13.hook_resid_mid', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_pre_linear', 'blocks.13.mlp.hook_post', 'blocks.13.ln2_post.hook_scale', 'blocks.13.ln2_post.hook_normalized', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_pre', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_rot_q', 'blocks.14.attn.hook_rot_k', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_z', 'blocks.14.ln1_post.hook_scale', 'blocks.14.ln1_post.hook_normalized', 'blocks.14.hook_attn_out', 'blocks.14.hook_resid_mid', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_pre_linear', 'blocks.14.mlp.hook_post', 'blocks.14.ln2_post.hook_scale', 'blocks.14.ln2_post.hook_normalized', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_pre', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_rot_q', 'blocks.15.attn.hook_rot_k', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_z', 'blocks.15.ln1_post.hook_scale', 'blocks.15.ln1_post.hook_normalized', 'blocks.15.hook_attn_out', 'blocks.15.hook_resid_mid', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_pre_linear', 'blocks.15.mlp.hook_post', 'blocks.15.ln2_post.hook_scale', 'blocks.15.ln2_post.hook_normalized', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_pre', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_rot_q', 'blocks.16.attn.hook_rot_k', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_z', 'blocks.16.ln1_post.hook_scale', 'blocks.16.ln1_post.hook_normalized', 'blocks.16.hook_attn_out', 'blocks.16.hook_resid_mid', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_pre_linear', 'blocks.16.mlp.hook_post', 'blocks.16.ln2_post.hook_scale', 'blocks.16.ln2_post.hook_normalized', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_pre', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_rot_q', 'blocks.17.attn.hook_rot_k', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_z', 'blocks.17.ln1_post.hook_scale', 'blocks.17.ln1_post.hook_normalized', 'blocks.17.hook_attn_out', 'blocks.17.hook_resid_mid', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_pre_linear', 'blocks.17.mlp.hook_post', 'blocks.17.ln2_post.hook_scale', 'blocks.17.ln2_post.hook_normalized', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_pre', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_rot_q', 'blocks.18.attn.hook_rot_k', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_z', 'blocks.18.ln1_post.hook_scale', 'blocks.18.ln1_post.hook_normalized', 'blocks.18.hook_attn_out', 'blocks.18.hook_resid_mid', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_pre_linear', 'blocks.18.mlp.hook_post', 'blocks.18.ln2_post.hook_scale', 'blocks.18.ln2_post.hook_normalized', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_pre', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_rot_q', 'blocks.19.attn.hook_rot_k', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_z', 'blocks.19.ln1_post.hook_scale', 'blocks.19.ln1_post.hook_normalized', 'blocks.19.hook_attn_out', 'blocks.19.hook_resid_mid', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_pre_linear', 'blocks.19.mlp.hook_post', 'blocks.19.ln2_post.hook_scale', 'blocks.19.ln2_post.hook_normalized', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_post.hook_sae_input', 'blocks.19.hook_resid_post.hook_sae_acts_pre', 'blocks.19.hook_resid_post.hook_sae_acts_post', 'blocks.19.hook_resid_post.hook_sae_recons', 'blocks.19.hook_resid_post.hook_sae_output', 'blocks.20.hook_resid_pre', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_rot_q', 'blocks.20.attn.hook_rot_k', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_z', 'blocks.20.ln1_post.hook_scale', 'blocks.20.ln1_post.hook_normalized', 'blocks.20.hook_attn_out', 'blocks.20.hook_resid_mid', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_pre_linear', 'blocks.20.mlp.hook_post', 'blocks.20.ln2_post.hook_scale', 'blocks.20.ln2_post.hook_normalized', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_pre', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_rot_q', 'blocks.21.attn.hook_rot_k', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_z', 'blocks.21.ln1_post.hook_scale', 'blocks.21.ln1_post.hook_normalized', 'blocks.21.hook_attn_out', 'blocks.21.hook_resid_mid', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_pre_linear', 'blocks.21.mlp.hook_post', 'blocks.21.ln2_post.hook_scale', 'blocks.21.ln2_post.hook_normalized', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_pre', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_rot_q', 'blocks.22.attn.hook_rot_k', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_z', 'blocks.22.ln1_post.hook_scale', 'blocks.22.ln1_post.hook_normalized', 'blocks.22.hook_attn_out', 'blocks.22.hook_resid_mid', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_pre_linear', 'blocks.22.mlp.hook_post', 'blocks.22.ln2_post.hook_scale', 'blocks.22.ln2_post.hook_normalized', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_pre', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_rot_q', 'blocks.23.attn.hook_rot_k', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_z', 'blocks.23.ln1_post.hook_scale', 'blocks.23.ln1_post.hook_normalized', 'blocks.23.hook_attn_out', 'blocks.23.hook_resid_mid', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_pre_linear', 'blocks.23.mlp.hook_post', 'blocks.23.ln2_post.hook_scale', 'blocks.23.ln2_post.hook_normalized', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_post', 'blocks.24.hook_resid_pre', 'blocks.24.ln1.hook_scale', 'blocks.24.ln1.hook_normalized', 'blocks.24.attn.hook_q', 'blocks.24.attn.hook_k', 'blocks.24.attn.hook_v', 'blocks.24.attn.hook_rot_q', 'blocks.24.attn.hook_rot_k', 'blocks.24.attn.hook_attn_scores', 'blocks.24.attn.hook_pattern', 'blocks.24.attn.hook_z', 'blocks.24.ln1_post.hook_scale', 'blocks.24.ln1_post.hook_normalized', 'blocks.24.hook_attn_out', 'blocks.24.hook_resid_mid', 'blocks.24.ln2.hook_scale', 'blocks.24.ln2.hook_normalized', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_pre_linear', 'blocks.24.mlp.hook_post', 'blocks.24.ln2_post.hook_scale', 'blocks.24.ln2_post.hook_normalized', 'blocks.24.hook_mlp_out', 'blocks.24.hook_resid_post', 'blocks.25.hook_resid_pre', 'blocks.25.ln1.hook_scale', 'blocks.25.ln1.hook_normalized', 'blocks.25.attn.hook_q', 'blocks.25.attn.hook_k', 'blocks.25.attn.hook_v', 'blocks.25.attn.hook_rot_q', 'blocks.25.attn.hook_rot_k', 'blocks.25.attn.hook_attn_scores', 'blocks.25.attn.hook_pattern', 'blocks.25.attn.hook_z', 'blocks.25.ln1_post.hook_scale', 'blocks.25.ln1_post.hook_normalized', 'blocks.25.hook_attn_out', 'blocks.25.hook_resid_mid', 'blocks.25.ln2.hook_scale', 'blocks.25.ln2.hook_normalized', 'blocks.25.mlp.hook_pre', 'blocks.25.mlp.hook_pre_linear', 'blocks.25.mlp.hook_post', 'blocks.25.ln2_post.hook_scale', 'blocks.25.ln2_post.hook_normalized', 'blocks.25.hook_mlp_out', 'blocks.25.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leb[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.2232e+01, -2.7003e+01,  4.6542e+00,  ..., -5.2398e+01,\n",
       "           2.0997e+01,  2.1721e-01],\n",
       "         [-6.7635e+00, -6.1891e+00, -1.1391e+01,  ..., -8.5298e+00,\n",
       "          -1.1453e+00, -1.9739e+00],\n",
       "         [-1.6946e+01, -4.8029e+00, -1.9131e+00,  ..., -4.0972e+00,\n",
       "           3.4327e+00, -5.6123e+00],\n",
       "         ...,\n",
       "         [-6.1531e+00, -4.0606e+00, -7.6024e+00,  ..., -2.5864e+00,\n",
       "          -3.1558e+00, -3.7671e+00],\n",
       "         [-6.5121e+00, -3.5599e+00, -7.1476e+00,  ..., -2.5776e+00,\n",
       "          -3.4720e+00, -3.5249e+00],\n",
       "         [-5.8592e+00, -3.5852e+00, -6.8130e+00,  ..., -3.1476e+00,\n",
       "          -3.7623e+00, -3.7608e+00]],\n",
       "\n",
       "        [[ 5.2232e+01, -2.7003e+01,  4.6542e+00,  ..., -5.2398e+01,\n",
       "           2.0997e+01,  2.1721e-01],\n",
       "         [-1.6245e+01,  2.8613e+00, -7.4969e+00,  ..., -6.8718e+00,\n",
       "          -1.0947e+01, -6.6743e+00],\n",
       "         [-1.6009e+01, -1.0201e+00, -9.5743e+00,  ..., -8.3371e+00,\n",
       "          -5.1224e+00, -3.5708e+00],\n",
       "         ...,\n",
       "         [-2.8493e+01, -7.9192e+00, -3.6725e+00,  ..., -5.8090e+00,\n",
       "           3.6836e+00,  7.3318e-01],\n",
       "         [-1.9946e+01,  1.6928e-02,  1.0463e+00,  ..., -7.0196e+00,\n",
       "          -2.9005e+00,  2.8219e+00],\n",
       "         [-1.9113e+01, -3.7103e+00, -3.5151e+00,  ..., -1.9463e+00,\n",
       "          -3.8823e-01,  9.6907e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leb[1]['blocks.19.hook_resid_post.hook_sae_acts_pre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = leb[1]['blocks.19.hook_resid_post.hook_sae_acts_post'][:,1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for h in hmm:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31.0618, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leb[1]['blocks.19.hook_resid_post.hook_sae_acts_post'][    1,     0,     263]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     4],\n",
       "        [    0,     0,   263],\n",
       "        [    0,     0,   774],\n",
       "        ...,\n",
       "        [    1,     7, 16209],\n",
       "        [    1,     7, 16303],\n",
       "        [    1,     7, 16372]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nonzero(leb[1]['blocks.19.hook_resid_post.hook_sae_acts_post'][:,1:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False,  ..., False,  True, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[ True, False, False,  ..., False,  True, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (leb[1]['blocks.19.hook_resid_post.hook_sae_acts_post'] > 0).nonzero(as_tuple=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_out = model.run_with_cache_with_saes(\n",
    "            batch_ids,\n",
    "            saes=sae,\n",
    "            names_filter=lambda name: name in [\n",
    "                'blocks.19.hook_resid_post.hook_sae_input',\n",
    "                'blocks.19.hook_resid_post.hook_sae_recons'\n",
    "            ]\n",
    "        )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
       "          -6.2675e+01,  2.9702e+01],\n",
       "         [-4.8884e+00, -6.8970e-01,  4.2473e-01,  ...,  2.6384e+00,\n",
       "           7.4499e-01,  1.6682e-01],\n",
       "         [-3.9028e+00, -8.6650e-01, -4.0607e+00,  ...,  1.5525e+00,\n",
       "           5.1970e+00, -1.1291e-01],\n",
       "         [-5.4336e+00,  1.4239e+00,  2.3322e+00,  ...,  1.5917e+00,\n",
       "          -5.3451e-01,  2.7083e+00],\n",
       "         [-9.6548e+00,  3.2607e+00,  4.4400e+00,  ..., -2.2930e+00,\n",
       "           1.9895e+00,  2.9883e+00]],\n",
       "\n",
       "        [[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
       "          -6.2675e+01,  2.9702e+01],\n",
       "         [-6.8911e+00,  8.6366e+00, -2.9912e+00,  ...,  7.0088e-01,\n",
       "          -1.5728e+00,  1.3779e+00],\n",
       "         [-4.2630e+00,  1.3513e+00, -2.4401e+00,  ..., -2.6403e+00,\n",
       "          -4.0709e+00,  3.4806e+00],\n",
       "         [-3.7446e+00, -6.9691e-01, -8.2747e-01,  ...,  5.4570e+00,\n",
       "          -1.1321e+00,  3.9979e-01],\n",
       "         [-6.1591e+00,  2.7186e+00, -5.2874e-01,  ..., -1.4442e+00,\n",
       "          -2.0987e+00,  1.7930e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leb[1]['blocks.19.hook_resid_post.hook_sae_recons']\n",
    "# tensor([[[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
    "        #   -6.2675e+01,  2.9702e+01],\n",
    "        #  [-4.8884e+00, -6.8970e-01,  4.2473e-01,  ...,  2.6384e+00,\n",
    "        #    7.4499e-01,  1.6682e-01],\n",
    "        #  [-3.9028e+00, -8.6650e-01, -4.0607e+00,  ...,  1.5525e+00,\n",
    "        #    5.1970e+00, -1.1291e-01],\n",
    "        #  [-5.4336e+00,  1.4239e+00,  2.3322e+00,  ...,  1.5917e+00,\n",
    "        #   -5.3451e-01,  2.7083e+00],\n",
    "        #  [-9.6548e+00,  3.2607e+00,  4.4400e+00,  ..., -2.2930e+00,\n",
    "        #    1.9895e+00,  2.9883e+00]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to('cuda')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(batches):\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other people's code\n",
    "\n",
    "labels = [\n",
    "    'has_alice',\n",
    "    'has_not',\n",
    "    'label',\n",
    "    'has_alice xor has_not',\n",
    "    'has_alice xor label',\n",
    "    'has_not xor label',\n",
    "    'has_alice xor has_not xor label',\n",
    "]\n",
    "    \n",
    "accs = {}\n",
    "for label in labels:\n",
    "    dm = DataManager()\n",
    "    for dataset in ['cities_alice', 'neg_cities_alice']:\n",
    "        dm.add_dataset(dataset, 'llama-2-13b-reset', 14, label=label, center=False, split=0.8)\n",
    "    acts, labels = dm.get('train')\n",
    "    probe = LRProbe.from_data(acts, labels, bias=True)\n",
    "    acts, labels = dm.get('val')\n",
    "    acc = (probe(acts).round() == labels).float().mean()\n",
    "    accs[label] = acc"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
