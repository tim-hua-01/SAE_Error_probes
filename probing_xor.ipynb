{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87507b56a1d94d8bb9f5f7bd4c9ea3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model2b = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>correct_country</th>\n",
       "      <th>has_alice</th>\n",
       "      <th>has_not</th>\n",
       "      <th>has_alice xor has_not</th>\n",
       "      <th>has_alice xor label</th>\n",
       "      <th>has_not xor label</th>\n",
       "      <th>has_alice xor has_not xor label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice: The city of Hegang is in China.</td>\n",
       "      <td>1</td>\n",
       "      <td>Hegang</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob: The city of Cangzhou is in China.</td>\n",
       "      <td>1</td>\n",
       "      <td>Cangzhou</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice: The city of Mogadishu is in Somalia.</td>\n",
       "      <td>1</td>\n",
       "      <td>Mogadishu</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob: The city of Laibin is in Ethiopia.</td>\n",
       "      <td>0</td>\n",
       "      <td>Laibin</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>China</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob: The city of Feira de Santana is in Brazil.</td>\n",
       "      <td>1</td>\n",
       "      <td>Feira de Santana</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alice: The city of Karachi is not in Pakistan.</td>\n",
       "      <td>0</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alice: The city of Luohe is in Brazil.</td>\n",
       "      <td>0</td>\n",
       "      <td>Luohe</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bob: The city of Tlalpan is not in Mexico.</td>\n",
       "      <td>0</td>\n",
       "      <td>Tlalpan</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bob: The city of Bekasi is in China.</td>\n",
       "      <td>0</td>\n",
       "      <td>Bekasi</td>\n",
       "      <td>China</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alice: The city of Langfang is not in India.</td>\n",
       "      <td>1</td>\n",
       "      <td>Langfang</td>\n",
       "      <td>India</td>\n",
       "      <td>China</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         statement  label              city  \\\n",
       "0           Alice: The city of Hegang is in China.      1            Hegang   \n",
       "1           Bob: The city of Cangzhou is in China.      1          Cangzhou   \n",
       "2      Alice: The city of Mogadishu is in Somalia.      1         Mogadishu   \n",
       "3          Bob: The city of Laibin is in Ethiopia.      0            Laibin   \n",
       "4  Bob: The city of Feira de Santana is in Brazil.      1  Feira de Santana   \n",
       "5   Alice: The city of Karachi is not in Pakistan.      0           Karachi   \n",
       "6           Alice: The city of Luohe is in Brazil.      0             Luohe   \n",
       "7       Bob: The city of Tlalpan is not in Mexico.      0           Tlalpan   \n",
       "8             Bob: The city of Bekasi is in China.      0            Bekasi   \n",
       "9     Alice: The city of Langfang is not in India.      1          Langfang   \n",
       "\n",
       "    country correct_country  has_alice  has_not  has_alice xor has_not  \\\n",
       "0     China           China       True    False                   True   \n",
       "1     China           China      False    False                  False   \n",
       "2   Somalia         Somalia       True    False                   True   \n",
       "3  Ethiopia           China      False    False                  False   \n",
       "4    Brazil          Brazil      False    False                  False   \n",
       "5  Pakistan        Pakistan       True     True                  False   \n",
       "6    Brazil           China       True    False                   True   \n",
       "7    Mexico          Mexico      False     True                   True   \n",
       "8     China       Indonesia      False    False                  False   \n",
       "9     India           China       True     True                  False   \n",
       "\n",
       "   has_alice xor label  has_not xor label  has_alice xor has_not xor label  \n",
       "0                False               True                            False  \n",
       "1                 True               True                             True  \n",
       "2                False               True                            False  \n",
       "3                False              False                            False  \n",
       "4                 True               True                             True  \n",
       "5                 True               True                            False  \n",
       "6                 True              False                             True  \n",
       "7                False               True                             True  \n",
       "8                False              False                            False  \n",
       "9                False              False                             True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_alice = pd.read_csv(\"cities_alice.csv\")\n",
    "neg_city_alice = pd.read_csv(\"neg_cities_alice.csv\")\n",
    "data = pd.concat([city_alice, neg_city_alice])\n",
    "#scramble rows\n",
    "np.random.seed(123)\n",
    "data = data.sample(frac = 1).reset_index(drop = True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_pytorch_tanh',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 16.0,\n",
       " 'attn_scores_soft_cap': 50.0,\n",
       " 'attn_types': ['global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local'],\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 256,\n",
       " 'd_mlp': 9216,\n",
       " 'd_model': 2304,\n",
       " 'd_vocab': 256000,\n",
       " 'd_vocab_out': 256000,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': 'cuda',\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-06,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'n_ctx': 8192,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 8,\n",
       " 'n_key_value_heads': 4,\n",
       " 'n_layers': 26,\n",
       " 'n_params': 2146959360,\n",
       " 'normalization_type': 'RMSPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'Gemma2ForCausalLM',\n",
       " 'output_logits_soft_cap': 30.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000.0,\n",
       " 'rotary_dim': 256,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'google/gemma-2-2b',\n",
       " 'tokenizer_prepends_bos': True,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': True,\n",
       " 'use_normalization_before_and_after': True,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': 4096}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2b.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = \"layer_19/width_16k/canonical\",\n",
    "    device = \"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247aa605c4774522b4e15f7cdba39715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Bing bong ding dong, I'm blowing this one, 9\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model2b.generate(\"Bing bong ding dong\", max_new_tokens = 10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_batch = model2b.tokenizer([\"Ding don bing bong\", \"King kong ding dong whoop whoop\"], return_tensors = \"pt\", padding = True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "leb = model2b.run_with_cache_with_saes(some_batch['input_ids'],saes = sae, names_filter = lambda name: name in ['blocks.19.hook_resid_post.hook_sae_recons', 'blocks.19.hook_resid_post.hook_sae_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
       "          -6.2675e+01,  2.9702e+01],\n",
       "         [-4.8884e+00, -6.8970e-01,  4.2473e-01,  ...,  2.6384e+00,\n",
       "           7.4499e-01,  1.6682e-01],\n",
       "         [-3.9028e+00, -8.6650e-01, -4.0607e+00,  ...,  1.5525e+00,\n",
       "           5.1970e+00, -1.1291e-01],\n",
       "         [-5.4336e+00,  1.4239e+00,  2.3322e+00,  ...,  1.5917e+00,\n",
       "          -5.3451e-01,  2.7083e+00],\n",
       "         [-9.6548e+00,  3.2607e+00,  4.4400e+00,  ..., -2.2930e+00,\n",
       "           1.9895e+00,  2.9883e+00]],\n",
       "\n",
       "        [[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
       "          -6.2675e+01,  2.9702e+01],\n",
       "         [-6.8911e+00,  8.6366e+00, -2.9912e+00,  ...,  7.0088e-01,\n",
       "          -1.5728e+00,  1.3779e+00],\n",
       "         [-4.2630e+00,  1.3513e+00, -2.4401e+00,  ..., -2.6403e+00,\n",
       "          -4.0709e+00,  3.4806e+00],\n",
       "         [-3.7446e+00, -6.9691e-01, -8.2747e-01,  ...,  5.4570e+00,\n",
       "          -1.1321e+00,  3.9979e-01],\n",
       "         [-6.1591e+00,  2.7186e+00, -5.2874e-01,  ..., -1.4442e+00,\n",
       "          -2.0987e+00,  1.7930e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leb[1]['blocks.19.hook_resid_post.hook_sae_recons']\n",
    "# tensor([[[ 6.3655e+01,  2.7575e+02, -1.7398e+02,  ...,  2.9820e+02,\n",
    "        #   -6.2675e+01,  2.9702e+01],\n",
    "        #  [-4.8884e+00, -6.8970e-01,  4.2473e-01,  ...,  2.6384e+00,\n",
    "        #    7.4499e-01,  1.6682e-01],\n",
    "        #  [-3.9028e+00, -8.6650e-01, -4.0607e+00,  ...,  1.5525e+00,\n",
    "        #    5.1970e+00, -1.1291e-01],\n",
    "        #  [-5.4336e+00,  1.4239e+00,  2.3322e+00,  ...,  1.5917e+00,\n",
    "        #   -5.3451e-01,  2.7083e+00],\n",
    "        #  [-9.6548e+00,  3.2607e+00,  4.4400e+00,  ..., -2.2930e+00,\n",
    "        #    1.9895e+00,  2.9883e+00]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to('cuda')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(batches):\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other people's code\n",
    "\n",
    "labels = [\n",
    "    'has_alice',\n",
    "    'has_not',\n",
    "    'label',\n",
    "    'has_alice xor has_not',\n",
    "    'has_alice xor label',\n",
    "    'has_not xor label',\n",
    "    'has_alice xor has_not xor label',\n",
    "]\n",
    "    \n",
    "accs = {}\n",
    "for label in labels:\n",
    "    dm = DataManager()\n",
    "    for dataset in ['cities_alice', 'neg_cities_alice']:\n",
    "        dm.add_dataset(dataset, 'llama-2-13b-reset', 14, label=label, center=False, split=0.8)\n",
    "    acts, labels = dm.get('train')\n",
    "    probe = LRProbe.from_data(acts, labels, bias=True)\n",
    "    acts, labels = dm.get('val')\n",
    "    acc = (probe(acts).round() == labels).float().mean()\n",
    "    accs[label] = acc"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
